# Day 7: Linear Regression Continued—Simple and Polynomial Models  

Today was all about mastering linear regression. I followed a great resource from [Microsoft’s ML for Beginners](https://github.com/microsoft/ML-For-Beginners/tree/main/2-Regression/3-Linear), which helped me into creating and experimenting with simple and polynomial regression models.  

---

## What I Did  

### 1. Simple Linear Regression  
- **Refresher**: Revisited the basics—modeling the relationship between a single feature (X) and a target (Y).  
- **Implementation**: Built a small regression model to predict outcomes using scikit-learn.  
- **Evaluation**: Measured performance using Mean Squared Error (MSE) and R-squared.  
- **Insights**: Found out how well a straight line can work for linear data trends.

### 2. Polynomial Regression  
- **Beyond Linear**: Learned how to fit curved data by extending simple regression to polynomial regression.  
- **Experimentation**: Tested different polynomial degrees (quadratic, cubic, etc.) to see how well they capture the trends.  
- **Overfitting Awareness**: Saw firsthand how using a high degree can make the model too specific and less generalizable.  

---

## Resource Used  
Big thanks to [Microsoft’s ML for Beginners - Linear Regression](https://github.com/microsoft/ML-For-Beginners/tree/main/2-Regression/3-Linear). It had clear explanations and examples that made things easier to understand.  

---

## How It Went  
- Getting hands-on with regression models was satisfying.  
- Polynomial regression felt powerful but tricky—it’s a balancing act to avoid overfitting while still capturing enough of the trend.  

---

## Exercises  
My simple and polynomial regression models are saved in the **[Problem Solving](../execrises/)** folder. Feel free to explore them (and judge my experiments).  

---

## What’s Next?  
Next, I’ll look into multiple linear regression and dive deeper into advanced evaluation metrics. Stay tuned!  

---
